{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e31fb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opencv-python\n",
    "%pip install numpy\n",
    "%pip install torch torchvision\n",
    "%pip install wget\n",
    "%pip install pytorchvideo\n",
    "%pip install json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24ed6f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nyok\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\nyok\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\nyok\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# IMPORT ALL THE PACKAGE\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import time\n",
    "import wget\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "from ipywidgets import Image\n",
    "\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f92771bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\nyok/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\n"
     ]
    }
   ],
   "source": [
    "# DOWNLOAD THE MODEL\n",
    "\n",
    "# Device on which to run the model\n",
    "# Set to cuda to load on GPU\n",
    "device = \"cpu\"\n",
    "\n",
    "# Pick a pretrained model and load the pretrained weights\n",
    "model_name = \"slowfast_r50\"\n",
    "model = torch.hub.load(\"facebookresearch/pytorchvideo\", model=model_name, pretrained=True)\n",
    "\n",
    "# Set to eval mode and move to desired device\n",
    "model = model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c70d7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"sharpening knives\"', '\"eating ice cream\"', '\"cutting nails\"', '\"changing wheel\"', '\"bench pressing\"', 'deadlifting', '\"eating carrots\"', 'marching', '\"throwing discus\"', '\"playing flute\"'] (400,)\n"
     ]
    }
   ],
   "source": [
    "# DOWNLOAD KINETIC-400 LABEL\n",
    "\n",
    "# url = \"https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json\"\n",
    "# wget.download(url, 'kinetics_classnames.json')\n",
    "\n",
    "with open(\"kinetics_classnames.json\", \"r\") as f:\n",
    "    kinetics_classnames = json.load(f)\n",
    "    labels = [line.strip() for line in kinetics_classnames]\n",
    "\n",
    "# Create an id to label name mapping\n",
    "kinetics_id_to_classname = {}\n",
    "for k, v in kinetics_classnames.items():\n",
    "    kinetics_id_to_classname[v] = str(k).replace('\"', \"\")\n",
    "\n",
    "print(labels[0:10], np.shape(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8269c6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORM PARAMETER\n",
    "side_size = 256\n",
    "mean = [0.45, 0.45, 0.45]\n",
    "std = [0.225, 0.225, 0.225]\n",
    "num_frames = 32\n",
    "sampling_rate = 2\n",
    "frames_per_second = 16\n",
    "slowfast_alpha = 4\n",
    "\n",
    "# TRANSFORMING THE VIDEO \n",
    "class PackPathway(torch.nn.Module):\n",
    "    def __init__(self, alpha=4):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, frames: torch.Tensor):\n",
    "        fast_pathway = frames\n",
    "        slow_pathway = torch.index_select(\n",
    "            frames,\n",
    "            1,\n",
    "            torch.linspace(0, frames.shape[1] - 1, frames.shape[1] // self.alpha).long(),\n",
    "        )\n",
    "        return slow_pathway, fast_pathway\n",
    "\n",
    "transform = ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=Compose(\n",
    "        [\n",
    "            UniformTemporalSubsample(num_frames),\n",
    "            Lambda(lambda x: x/255.0),\n",
    "            NormalizeVideo(mean, std),\n",
    "            ShortSideScale(size=side_size),\n",
    "            PackPathway(slowfast_alpha)\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c5c2e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN PRE-PROCESSING FUNCTION\n",
    "\n",
    "def process_video(use_webcam=False, video_path=None):\n",
    "    if use_webcam:\n",
    "        capture = cv2.VideoCapture(0)\n",
    "    elif video_path:\n",
    "        capture = cv2.VideoCapture(video_path)\n",
    "    else:\n",
    "        print(\"Error: Please specify either use_webcam=True or provide a video_path.\")\n",
    "        return\n",
    "\n",
    "    if not capture.isOpened():\n",
    "        print(\"Error: Could not open video source.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            frames = []\n",
    "            for _ in range(num_frames):\n",
    "                ret, img = capture.read()\n",
    "                if not ret:\n",
    "                    print(\"Error: Could not read frame.\")\n",
    "                    break\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                img = cv2.resize(img, (side_size, side_size))\n",
    "                frames.append(img)\n",
    "\n",
    "            if len(frames) < num_frames:\n",
    "                break\n",
    "\n",
    "            # Create input tensor\n",
    "            video_tensor = torch.tensor(np.array(frames)).permute(3, 0, 1, 2).float()\n",
    "            video_data = {\"video\": video_tensor}\n",
    "\n",
    "            # Apply the transform to normalize the input\n",
    "            video_data = transform(video_data)\n",
    "\n",
    "            # Prepare inputs for the model\n",
    "            slow_pathway, fast_pathway = video_data[\"video\"]\n",
    "            inputs = [slow_pathway.unsqueeze(0), fast_pathway.unsqueeze(0)]\n",
    "            # inputs = [i.to(device) for i in inputs]  # Uncomment when using GPU\n",
    "\n",
    "            # Pass the input through the model\n",
    "            with torch.no_grad():\n",
    "                preds = model(inputs)\n",
    "\n",
    "            # Apply softmax to get class probabilities\n",
    "            post_act = torch.nn.Softmax(dim=1)\n",
    "            preds = post_act(preds)\n",
    "            pred_classes = preds.topk(k=5).indices[0]\n",
    "\n",
    "            # Map the predicted classes to the label names\n",
    "            pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes]\n",
    "\n",
    "            # Display the prediction on the video frame\n",
    "            img_bgr = cv2.cvtColor(frames[-1], cv2.COLOR_RGB2BGR)\n",
    "            for i, class_name in enumerate(pred_class_names):\n",
    "                cv2.putText(img_bgr, f\"{class_name}\", (20, 40 + i * 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "            # Show the video stream with predictions\n",
    "            cv2.imshow('SlowFast R50 Action Recognition', img_bgr)\n",
    "\n",
    "            # Press 'Esc' to exit\n",
    "            if cv2.waitKey(30) & 0xFF == 27:\n",
    "                break\n",
    "\n",
    "    finally:\n",
    "        capture.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02d8b90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RUNNING THE MODEL WITH OR WITHOUT WEBCAME\n",
    "# For webcam:\n",
    "process_video(use_webcam=True)\n",
    "\n",
    "# For video file:\n",
    "# process_video(video_path=\"C:/Users/nyok/Desktop/OpenCV/Videos/diving.MP4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e6f62e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
