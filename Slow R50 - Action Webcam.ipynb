{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19a58103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nyok\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\nyok\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\nyok\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# IMPORT ALL PACKAGE\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import time\n",
    "import wget\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "from ipywidgets import Image\n",
    "\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47894432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\nyok/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\n"
     ]
    }
   ],
   "source": [
    "# DOWNLOAD MODEL\n",
    "\n",
    "# Device on which to run the model\n",
    "# Set to cuda to load on GPU\n",
    "device = \"cpu\"\n",
    "\n",
    "# Pick a pretrained model and load the pretrained weights\n",
    "model_name = \"slow_r50\"\n",
    "model = torch.hub.load(\"facebookresearch/pytorchvideo\", model=model_name, pretrained=True)\n",
    "\n",
    "# Set to eval mode and move to desired device\n",
    "model = model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5917d403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"sharpening knives\"', '\"eating ice cream\"', '\"cutting nails\"', '\"changing wheel\"', '\"bench pressing\"', 'deadlifting', '\"eating carrots\"', 'marching', '\"throwing discus\"', '\"playing flute\"'] (400,)\n"
     ]
    }
   ],
   "source": [
    "# DOWNLOAD KINETIC-400 LABEL\n",
    "# url = \"https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json\"\n",
    "# wget.download(url, 'kinetics_classnames.json')\n",
    "\n",
    "with open(\"kinetics_classnames.json\", \"r\") as f:\n",
    "    kinetics_classnames = json.load(f)\n",
    "    labels = [line.strip() for line in kinetics_classnames]\n",
    "\n",
    "# Create an id to label name mapping\n",
    "kinetics_id_to_classname = {}\n",
    "for k, v in kinetics_classnames.items():\n",
    "    kinetics_id_to_classname[v] = str(k).replace('\"', \"\")\n",
    "\n",
    "print(labels[0:10], np.shape(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcd28088",
   "metadata": {},
   "outputs": [],
   "source": [
    "side_size = 480\n",
    "mean = [0.45, 0.45, 0.45]\n",
    "std = [0.225, 0.225, 0.225]\n",
    "crop_size = 480\n",
    "num_frames = 8\n",
    "sampling_rate = 8\n",
    "frames_per_second = 16\n",
    "\n",
    "# Note that this transform is specific to the slow_R50 model.\n",
    "transform =  ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=Compose(\n",
    "        [\n",
    "            UniformTemporalSubsample(num_frames),\n",
    "            Lambda(lambda x: x/255.0),\n",
    "            NormalizeVideo(mean, std),\n",
    "            ShortSideScale(\n",
    "                size=side_size\n",
    "            ),\n",
    "            CenterCropVideo(crop_size=(crop_size, crop_size))\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# The duration of the input clip is also specific to the model.\n",
    "clip_duration = (num_frames * sampling_rate)/frames_per_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c016dece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN PROCESSING FUNCTION\n",
    "def process_video(use_webcam=False, video_path=None):\n",
    "    # Initialize webcam or video file\n",
    "    if use_webcam:\n",
    "        capture = cv2.VideoCapture(0)\n",
    "    elif video_path:\n",
    "        capture = cv2.VideoCapture(video_path)\n",
    "    else:\n",
    "        print(\"Error: Provide video path or function for using webcam\")\n",
    "        return\n",
    "    if not capture.isOpened():\n",
    "        print(\"Error: Could not open video source.\")\n",
    "        return\n",
    "\n",
    "    prev_time = 0\n",
    "    start_time = time.time()  \n",
    "    frame_count = 0\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            frames = []\n",
    "            for _ in range(num_frames):\n",
    "                ret, img = capture.read()\n",
    "                if not ret:\n",
    "                    print(\"Error: Could not read frame.\")\n",
    "                    break\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                img = cv2.resize(img, (side_size, side_size))\n",
    "                frames.append(img)\n",
    "\n",
    "            if len(frames) < num_frames:\n",
    "                break\n",
    "            frame_count += 1\n",
    "\n",
    "            # Create input tensor\n",
    "            video_tensor = torch.tensor(np.array(frames)).permute(3, 0, 1, 2).float()\n",
    "            video_data = {\"video\": video_tensor}\n",
    "\n",
    "            # Apply the transform to normalize the input\n",
    "            video_data = transform(video_data)\n",
    "\n",
    "            # Move the inputs to the desired device\n",
    "            inputs = video_data[\"video\"].to(device)\n",
    "\n",
    "            # Pass the input through the model\n",
    "            with torch.no_grad():\n",
    "                preds = model(inputs[None, ...])\n",
    "\n",
    "\n",
    "            # Apply softmax to get class probabilities\n",
    "            post_act = torch.nn.Softmax(dim=1)\n",
    "            preds = post_act(preds)\n",
    "            pred_classes = preds.topk(k=3).indices[0]\n",
    "\n",
    "            # Map the predicted classes to the label names\n",
    "            pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes]\n",
    "\n",
    "            # Display the prediction on the video frame\n",
    "            img_bgr = cv2.cvtColor(frames[-1], cv2.COLOR_RGB2BGR)\n",
    "            for i, class_name in enumerate(pred_class_names):\n",
    "                cv2.putText(img_bgr, f\"{class_name}\", (20, 40 + i * 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "                \n",
    "            current_time = time.time()\n",
    "            total_time = current_time - start_time  # Total elapsed time since the start\n",
    "            \n",
    "            # Calculate FPS (instantaneous and average)\n",
    "            fps = 1 / (current_time - prev_time) if (current_time - prev_time) > 0 else 0\n",
    "            average_fps = frame_count / total_time if total_time > 0 else 0\n",
    "            prev_time = current_time\n",
    "                \n",
    "            cv2.putText(img_bgr, f\"FPS: {int(fps)}\", (20, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "            cv2.putText(img_bgr, f\"Average FPS: {int(average_fps)}\", (20, 140), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "            # Show the video stream with predictions\n",
    "            cv2.imshow('Slow Model Action Recognition', img_bgr)\n",
    "\n",
    "            # Press 'Esc' to exit\n",
    "            if cv2.waitKey(30) & 0xFF == 27:\n",
    "                break\n",
    "\n",
    "    finally:\n",
    "        capture.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8964e6b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# RUNNING THE MODEL WITH OR WITHOUT WEBCAME\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# For webcam:\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m process_video(use_webcam\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m, in \u001b[0;36mprocess_video\u001b[1;34m(use_webcam, video_path)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_video\u001b[39m(use_webcam\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, video_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Initialize webcam or video file\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_webcam:\n\u001b[1;32m----> 5\u001b[0m         capture \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m video_path:\n\u001b[0;32m      7\u001b[0m         capture \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(video_path)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "# RUNNING THE MODEL WITH OR WITHOUT WEBCAME\n",
    "# For webcam:\n",
    "process_video(use_webcam=True)\n",
    "\n",
    "# For video file:\n",
    "# process_video(video_path=\"C:/Users/nyok/Desktop/OpenCV/Videos/eating.MP4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7684c084",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
