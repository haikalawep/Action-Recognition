{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95840026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.5\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python\n",
    "%pip install numpy\n",
    "%pip install torch torchvision\n",
    "%pip install wget\n",
    "%pip install pytorchvideo\n",
    "%pip install json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "995f36cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.video import mc3_18, MC3_18_Weights\n",
    "\n",
    "\n",
    "# IMPORT DETECTION PACKAGE\n",
    "from PIL import Image, ImageDraw\n",
    "from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import pil_to_tensor, to_pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d75a7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# DOWNLOAD THE MODEL\n",
    "\n",
    "# Device on which to run the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Step 1: Initialize the model with the best available weights\n",
    "weights = MC3_18_Weights.DEFAULT\n",
    "model_mc3_18 = mc3_18(weights=weights)\n",
    "model_mc3_18.eval()\n",
    "model_mc3_18 = model_mc3_18.to(device)\n",
    "\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1f991c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"sharpening knives\"', '\"eating ice cream\"', '\"cutting nails\"', '\"changing wheel\"', '\"bench pressing\"', 'deadlifting', '\"eating carrots\"', 'marching', '\"throwing discus\"', '\"playing flute\"'] (400,)\n"
     ]
    }
   ],
   "source": [
    "# DOWNLOAD THE KINETIC-400 LABEL\n",
    "\n",
    "# url = \"https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json\"\n",
    "# wget.download(url, 'kinetics_classnames.json')\n",
    "\n",
    "with open(\"kinetics_classnames.json\", \"r\") as f:\n",
    "    kinetics_classnames = json.load(f)\n",
    "    labels = [line.strip() for line in kinetics_classnames]\n",
    "\n",
    "# Create an id to label name mapping\n",
    "kinetics_id_to_classname = {}\n",
    "for k, v in kinetics_classnames.items():\n",
    "    kinetics_id_to_classname[v] = str(k).replace('\"', \"\")\n",
    "\n",
    "print(labels[0:10], np.shape(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "047c9620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMBINE THE DETECTION AND RECOGNITION\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "weights_detect = FasterRCNN_MobileNet_V3_Large_320_FPN_Weights.DEFAULT\n",
    "model_detection = fasterrcnn_mobilenet_v3_large_320_fpn(weights=weights_detect, box_score_thresh=0.9)\n",
    "model_detection.eval()\n",
    "\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess_detect = weights_detect.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9eb34b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DETECTION FUNCTION\n",
    "def detect(img):\n",
    "    # Convert OpenCV BGR frame to PIL Image\n",
    "    pil_img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Convert PIL Image to tensor and add batch dimension\n",
    "    img_tensor = pil_to_tensor(pil_img).unsqueeze(0)\n",
    "\n",
    "    # Apply inference preprocessing transforms\n",
    "    batch = [preprocess_detect(img_tensor[0])]\n",
    "\n",
    "    # Step 5: Perform object detection\n",
    "    with torch.no_grad():\n",
    "        prediction = model_detection(batch)[0]\n",
    "\n",
    "    # Extract labels and draw bounding boxes on the frame\n",
    "    boxes = prediction[\"boxes\"]\n",
    "    box = draw_bounding_boxes(img_tensor[0], boxes=boxes, colors=\"red\", width=2)\n",
    "\n",
    "    # Convert tensor back to OpenCV format for display\n",
    "    result_frame = cv2.cvtColor(np.array(to_pil_image(box.detach())), cv2.COLOR_RGB2BGR)\n",
    "    return result_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "605db091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize(video_tensor):\n",
    "    video_tensor = video_tensor.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model_mc3_18(video_tensor).squeeze(0).softmax(0)\n",
    "    label = prediction.argmax().item()\n",
    "    score = prediction[label].item()\n",
    "    category_name = weights.meta[\"categories\"][label]\n",
    "    print(f\"{category_name}: {100 * score:.2f}%\")\n",
    "    \n",
    "    return category_name, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d73cdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN PROCESSING FUNCTION\n",
    "def process_video(use_webcam=False, video_path=None):\n",
    "\n",
    "    # Initialize webcam or video file\n",
    "    if use_webcam:\n",
    "        capture = cv2.VideoCapture(0)\n",
    "    elif video_path:\n",
    "        capture = cv2.VideoCapture(video_path)\n",
    "    else:\n",
    "        print(\"Error: Provide video path or function for using webcam\")\n",
    "        return\n",
    "    if not capture.isOpened():\n",
    "        print(\"Error: Could not open video source.\")\n",
    "        return\n",
    "    \n",
    "    # Step 4: Initialize a frame buffer to collect frames for prediction\n",
    "    frame_buffer = []\n",
    "    num_frames_to_process = 4  # Number of frames needed for action recognition\n",
    "    prev_time = 0\n",
    "    start_time = time.time()\n",
    "    frame_count = 0\n",
    "    input_size = (480, 480)\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            frames = []\n",
    "            for _ in range(num_frames_to_process):\n",
    "                ret, img = capture.read()\n",
    "                if not ret:\n",
    "                    print(\"Error: Could not read frame.\")\n",
    "                    break\n",
    "                    \n",
    "                frame_count += 1\n",
    "                \n",
    "                resized_frame = cv2.resize(img, input_size)\n",
    "                \n",
    "                detect_img = detect(resized_frame)\n",
    "                print(f\"Original shape of video: {detect_img.shape}\") \n",
    "                \n",
    "                frames.append(cv2.cvtColor(detect_img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "            if len(frames) < num_frames_to_process:\n",
    "                break\n",
    "\n",
    "            video_tensor = torch.tensor(np.array(frames)).permute(0, 3, 1, 2).float() / 255.0  # Normalize values between 0 and 1\n",
    "\n",
    "            # Apply the transform to normalize the input\n",
    "            video_tensor = preprocess(video_tensor).unsqueeze(0)  # Add batch dimension\n",
    "            print(f\"After of inputs: {video_tensor.shape}\")\n",
    "            \n",
    "            # Move tensor to the device\n",
    "            (category_name, score) = recognize(video_tensor)\n",
    "\n",
    "            cv2.putText(detect_img, f\"{category_name}: {100 * score:.2f}%\", (20, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 153, 0), 2)\n",
    "            \n",
    "            current_time = time.time()\n",
    "            total_time = current_time - start_time  # Total elapsed time since the start\n",
    "\n",
    "            # Calculate FPS (instantaneous and average)\n",
    "            fps = 1 / (current_time - prev_time) if (current_time - prev_time) > 0 else 0\n",
    "            average_fps = frame_count / total_time if total_time > 0 else 0\n",
    "            prev_time = current_time\n",
    "\n",
    "            cv2.putText(detect_img, f\"FPS: {float(fps):.2f}\", (20, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "            cv2.putText(detect_img, f\"Average FPS: {float(average_fps):.2f}\", (20, 140), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "            # Show the video stream with predictions\n",
    "            cv2.imshow('MC3 Action Recognition', detect_img)\n",
    "\n",
    "            # Press 'Esc' to exit\n",
    "            if cv2.waitKey(30) & 0xFF == 27:\n",
    "                break\n",
    "\n",
    "    finally:\n",
    "        capture.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62546f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape of video: (480, 480, 3)\n",
      "Original shape of video: (480, 480, 3)\n",
      "Original shape of video: (480, 480, 3)\n",
      "Original shape of video: (480, 480, 3)\n",
      "After of inputs: torch.Size([1, 3, 4, 112, 112])\n",
      "pumping gas: 66.65%\n",
      "Original shape of video: (480, 480, 3)\n",
      "Original shape of video: (480, 480, 3)\n",
      "Original shape of video: (480, 480, 3)\n",
      "Original shape of video: (480, 480, 3)\n",
      "After of inputs: torch.Size([1, 3, 4, 112, 112])\n",
      "grooming dog: 43.95%\n",
      "Original shape of video: (480, 480, 3)\n",
      "Original shape of video: (480, 480, 3)\n",
      "Original shape of video: (480, 480, 3)\n",
      "Original shape of video: (480, 480, 3)\n",
      "After of inputs: torch.Size([1, 3, 4, 112, 112])\n",
      "pumping gas: 64.92%\n"
     ]
    }
   ],
   "source": [
    "# RUNNING THE MODEL WITH OR WITHOUT WEBCAME\n",
    "# For webcam:\n",
    "process_video(use_webcam=True)\n",
    "\n",
    "# For video file:\n",
    "# process_video(video_path=\"C:/Users/nyok/Desktop/OpenCV/Videos/eatinglive.MOV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7040c64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating doughnuts: 53.52%\n",
      "eating doughnuts: 64.22%\n",
      "eating doughnuts: 88.42%\n",
      "eating burger: 53.14%\n",
      "eating doughnuts: 69.54%\n",
      "tasting food: 60.27%\n",
      "eating doughnuts: 50.71%\n",
      "waxing eyebrows: 60.24%\n",
      "waxing eyebrows: 67.35%\n",
      "tasting food: 41.86%\n",
      "tasting food: 43.13%\n",
      "tasting food: 57.04%\n",
      "tasting food: 36.97%\n",
      "curling hair: 32.92%\n",
      "playing volleyball: 50.89%\n",
      "curling hair: 38.04%\n",
      "playing volleyball: 67.22%\n",
      "playing ice hockey: 48.75%\n",
      "playing ice hockey: 60.00%\n",
      "tasting food: 51.89%\n",
      "tasting food: 74.04%\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5366353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6c2ee5-df0d-45c3-866f-c29937530110",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
