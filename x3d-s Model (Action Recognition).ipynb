{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3c1c3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: You must give at least one requirement to install (see \"pip help install\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: You must give at least one requirement to install (see \"pip help install\")\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python\n",
    "%pip install numpy\n",
    "%pip install torch torchvision\n",
    "%pip install wget\n",
    "%pip install pytorchvideo\n",
    "%pip install json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9f51364",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nyok\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\nyok\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\nyok\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS ALL PACKAGE\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import wget\n",
    "import os\n",
    "\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample\n",
    ")\n",
    "\n",
    "# IMPORT DETECTION PACKAGE\n",
    "from PIL import Image, ImageDraw\n",
    "from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import pil_to_tensor, to_pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07aee90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\nyok/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\n"
     ]
    }
   ],
   "source": [
    "# DOWNLOAD THE RECOGNITION MODEL\n",
    "\n",
    "# Device on which to run the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Pick a pretrained model and load the pretrained weights\n",
    "model_name = \"x3d_xs\"\n",
    "model_recognition = torch.hub.load(\"facebookresearch/pytorchvideo\", model=model_name, pretrained=True)\n",
    "\n",
    "# Set to eval mode and move to desired device\n",
    "model_recognition = model_recognition.eval()\n",
    "model_recognition = model_recognition.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e116ed7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kinetics_classnames.json is already in directory. Skipping Download\n",
      "['\"sharpening knives\"', '\"eating ice cream\"', '\"cutting nails\"', '\"changing wheel\"', '\"bench pressing\"', 'deadlifting', '\"eating carrots\"', 'marching', '\"throwing discus\"', '\"playing flute\"'] (400,)\n"
     ]
    }
   ],
   "source": [
    "# DOWNLOAD THE KINETIC-400 LABEL\n",
    "\n",
    "url = \"https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json\"\n",
    "file_name = \"kinetics_classnames.json\"\n",
    "\n",
    "if os.path.isfile(file_name):\n",
    "    print(f\"{file_name} is already in directory. Skipping Download\")\n",
    "else:\n",
    "    print(\"Downloading kinetics__classnames\")\n",
    "    wget.download(url, \"kinetics_classnames.json\")\n",
    "\n",
    "with open(\"kinetics_classnames.json\", \"r\") as f:\n",
    "    kinetics_classnames = json.load(f)\n",
    "    labels = [line.strip() for line in kinetics_classnames]\n",
    "\n",
    "# Create an id to label name mapping\n",
    "kinetics_id_to_classname = {}\n",
    "for k, v in kinetics_classnames.items():\n",
    "    kinetics_id_to_classname[v] = str(k).replace('\"', \"\")\n",
    "\n",
    "print(labels[0:10], np.shape(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2197f35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORM PARAMETER\n",
    "mean = [0.45, 0.45, 0.45]\n",
    "std = [0.225, 0.225, 0.225]\n",
    "frames_per_second = 6\n",
    "\n",
    "# SPECIFIC PARAMETER FOR MODEL\n",
    "model_transform_params = {\n",
    "    \"x3d_xs\": {\"side_size\": 182, \"crop_size\": 182, \"num_frames\": 4, \"sampling_rate\": 12},\n",
    "    \"x3d_s\": {\"side_size\": 182, \"crop_size\": 182, \"num_frames\": 13, \"sampling_rate\": 6},\n",
    "    \"x3d_m\": {\"side_size\": 256, \"crop_size\": 256, \"num_frames\": 16, \"sampling_rate\": 5}\n",
    "}\n",
    "\n",
    "transform_params = model_transform_params[model_name]\n",
    "\n",
    "transform = ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=Compose(\n",
    "        [\n",
    "            UniformTemporalSubsample(transform_params[\"num_frames\"]),\n",
    "            Lambda(lambda x: x/255.0),\n",
    "            NormalizeVideo((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "            ShortSideScale(size=transform_params[\"side_size\"]),\n",
    "            CenterCropVideo(\n",
    "                crop_size=(transform_params[\"crop_size\"], transform_params[\"crop_size\"])\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# UniformTemporalSubsample : Reduces the number of frames to the required num_frames by uniformly sampling.\n",
    "# Lambda(lambda x: x/255.0): Normalizes pixel values to the range [0, 1].\n",
    "# NormalizeVideo           : Applies mean and standard deviation normalization using the provided values.\n",
    "# ShortSideScale           : Resizes the video frames so that the shorter side is of length side_size.\n",
    "# CenterCropVideo          : Crops the center crop_sizeÃ—crop_size region from each frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22c7b319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMBINE THE DETECTION AND RECOGNITION\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "weights = FasterRCNN_MobileNet_V3_Large_320_FPN_Weights.DEFAULT\n",
    "model_detection = fasterrcnn_mobilenet_v3_large_320_fpn(weights=weights, box_score_thresh=0.9)\n",
    "model_detection.eval()\n",
    "\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6df980f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE MODEL WITH ACCURACY SCORE FOR EACH FRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c795b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DETECTION FUNCTION\n",
    "def detects(resized_frame):\n",
    "    # Convert OpenCV BGR frame to PIL Image\n",
    "    pil_img = Image.fromarray(cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Convert PIL Image to tensor and add batch dimension\n",
    "    img_tensor = pil_to_tensor(pil_img).unsqueeze(0)\n",
    "\n",
    "    # Apply inference preprocessing transforms\n",
    "    batch = [preprocess(img_tensor[0])]\n",
    "\n",
    "    # Step 5: Perform object detection\n",
    "    with torch.no_grad():\n",
    "        prediction = model_detection(batch)[0]\n",
    "\n",
    "    # Draw bounding boxes on the frame\n",
    "    boxes = prediction[\"boxes\"]\n",
    "    box = draw_bounding_boxes(img_tensor[0], boxes=boxes, colors=\"red\", width=2)\n",
    "\n",
    "    # Convert tensor back to OpenCV format for display\n",
    "    result_frame = cv2.cvtColor(np.array(to_pil_image(box.detach())), cv2.COLOR_RGB2BGR)\n",
    "    return result_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad76dc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognizes(video_data):\n",
    "    # Move the inputs to the desired device\n",
    "    inputs = video_data[\"video\"].to(device).unsqueeze(0)\n",
    "\n",
    "    print(f\"Shape of inputs: {inputs.shape}\")\n",
    "\n",
    "    # Pass the input through the model\n",
    "    with torch.no_grad():\n",
    "        prediction = model_recognition(inputs).squeeze(0).softmax(0)\n",
    "        # Get the top prediction (label with the highest score)\n",
    "        label = prediction.argmax().item()\n",
    "        score = prediction[label].item()\n",
    "\n",
    "        # Print the top prediction and its confidence score\n",
    "        print(f\"Top prediction: {kinetics_id_to_classname[label]} ({100 * score:.2f}%)\")\n",
    "        pred_class = kinetics_id_to_classname[label]\n",
    "\n",
    "    return pred_class, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2618f707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN PROCESSING FUNCTION\n",
    "def process_videos(use_webcam=False, video_path=None):\n",
    "    # Initialize webcam or video file\n",
    "    if use_webcam:\n",
    "        capture = cv2.VideoCapture(0)\n",
    "    elif video_path:\n",
    "        capture = cv2.VideoCapture(video_path)\n",
    "    else:\n",
    "        print(\"Error: Provide video path or function for using webcam\")\n",
    "        return\n",
    "    if not capture.isOpened():\n",
    "        print(\"Error: Could not open video source.\")\n",
    "        return\n",
    "    \n",
    "    input_size = (480, 480)\n",
    "    frame_buffer = []\n",
    "    frame_count = 0\n",
    "    prev_time = 0\n",
    "    start_time = time.time()   \n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            ret, img = capture.read()\n",
    "            if not ret:\n",
    "                print(\"Error: Could not read frame.\")\n",
    "                break\n",
    "            \n",
    "            frame_count += 1\n",
    "            \n",
    "            # Skip frames based on the value of skip_frames\n",
    "#             if frame_count % 2 != 0:\n",
    "#                 continue \n",
    "            \n",
    "            # Resize image based on input size\n",
    "            resized_frame = cv2.resize(img, input_size)\n",
    "            \n",
    "            # Call detection function \n",
    "            result_frame = detects(resized_frame)\n",
    "            \n",
    "            # Add frame to buffer\n",
    "            frame_buffer.append(cv2.cvtColor(result_frame, cv2.COLOR_BGR2RGB))\n",
    "            if len(frame_buffer) >= transform_params[\"num_frames\"]:\n",
    "                try:\n",
    "                    # Create a batch of repeated frames to match with required number for model\n",
    "                    video_frames = frame_buffer[-transform_params[\"num_frames\"]:]\n",
    "\n",
    "                    # Convert to PyTorch tensor and permute the tensor into shape (C,T,H,W)\n",
    "                    video_data = {\"video\": torch.tensor(np.array(video_frames)).permute(3, 0, 1, 2).float()}\n",
    "\n",
    "                    print(f\"Shape of video_data['video']: {video_data['video'].shape}\")\n",
    "\n",
    "                    # Apply the transform to normalize the input for model\n",
    "                    video_data = transform(video_data)\n",
    "                    \n",
    "                    (pred_class, score) = recognizes(video_data)\n",
    "                    \n",
    "                    print(pred_class, score)\n",
    "                    \n",
    "                    cv2.putText(result_frame, f\"{pred_class} ({10000 * score:.2f}%)\", (20, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 153, 0), 2)\n",
    "                    153\n",
    "#                     print(predict_class, score)\n",
    "\n",
    "                    # Display the prediction on the video frame\n",
    "#                     for i, class_name in enumerate(label):\n",
    "#                         cv2.putText(result_frame, f\"{class_name}\", (20, 40 + i * 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "                        \n",
    "                except RuntimeError as e:\n",
    "                    print(f\"RuntimeError in action recognition: {e}\")\n",
    "                    print(f\"Error occurred at frame {frame_count}\")\n",
    "        \n",
    "        \n",
    "            current_time = time.time()\n",
    "            total_time = current_time - start_time  # Total elapsed time since the start\n",
    "            \n",
    "            # Calculate FPS (instantaneous and average)\n",
    "            fps = 1 / (current_time - prev_time) if (current_time - prev_time) > 0 else 0\n",
    "            average_fps = frame_count / total_time if total_time > 0 else 0\n",
    "            prev_time = current_time\n",
    "    \n",
    "        \n",
    "            cv2.putText(result_frame, f\"FPS: {float(fps):.2f}\", (20, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "            cv2.putText(result_frame, f\"Average FPS: {float(average_fps):.2f}\", (20, 140), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "            \n",
    "            # Show the video stream with predictions\n",
    "            cv2.imshow('X3D-S Action Recognition', result_frame)\n",
    "                            \n",
    "            # Press 'Esc' to exit\n",
    "            if cv2.waitKey(30) & 0xFF == 27:\n",
    "                break\n",
    "    finally:\n",
    "        capture.release()\n",
    "        cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7aa17ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of video_data['video']: torch.Size([3, 4, 480, 480])\n",
      "Shape of inputs: torch.Size([1, 3, 4, 182, 182])\n",
      "Top prediction: beatboxing (0.37%)\n",
      "beatboxing 0.003670202801004052\n",
      "Shape of video_data['video']: torch.Size([3, 4, 480, 480])\n",
      "Shape of inputs: torch.Size([1, 3, 4, 182, 182])\n",
      "Top prediction: beatboxing (0.33%)\n",
      "beatboxing 0.003289933083578944\n",
      "Shape of video_data['video']: torch.Size([3, 4, 480, 480])\n",
      "Shape of inputs: torch.Size([1, 3, 4, 182, 182])\n",
      "Top prediction: beatboxing (0.41%)\n",
      "beatboxing 0.004067048896104097\n",
      "Shape of video_data['video']: torch.Size([3, 4, 480, 480])\n",
      "Shape of inputs: torch.Size([1, 3, 4, 182, 182])\n",
      "Top prediction: playing harmonica (0.37%)\n",
      "playing harmonica 0.0036784049589186907\n",
      "Shape of video_data['video']: torch.Size([3, 4, 480, 480])\n",
      "Shape of inputs: torch.Size([1, 3, 4, 182, 182])\n",
      "Top prediction: eating burger (0.35%)\n",
      "eating burger 0.003452070290222764\n",
      "Shape of video_data['video']: torch.Size([3, 4, 480, 480])\n",
      "Shape of inputs: torch.Size([1, 3, 4, 182, 182])\n",
      "Top prediction: beatboxing (0.37%)\n",
      "beatboxing 0.003680088324472308\n",
      "Shape of video_data['video']: torch.Size([3, 4, 480, 480])\n",
      "Shape of inputs: torch.Size([1, 3, 4, 182, 182])\n",
      "Top prediction: playing harmonica (0.34%)\n",
      "playing harmonica 0.0034027344081550837\n",
      "Shape of video_data['video']: torch.Size([3, 4, 480, 480])\n",
      "Shape of inputs: torch.Size([1, 3, 4, 182, 182])\n",
      "Top prediction: beatboxing (0.31%)\n",
      "beatboxing 0.003067441051825881\n",
      "Shape of video_data['video']: torch.Size([3, 4, 480, 480])\n",
      "Shape of inputs: torch.Size([1, 3, 4, 182, 182])\n",
      "Top prediction: playing harmonica (0.36%)\n",
      "playing harmonica 0.0035995864309370518\n",
      "Shape of video_data['video']: torch.Size([3, 4, 480, 480])\n",
      "Shape of inputs: torch.Size([1, 3, 4, 182, 182])\n",
      "Top prediction: smoking (0.31%)\n",
      "smoking 0.0030702820513397455\n",
      "Shape of video_data['video']: torch.Size([3, 4, 480, 480])\n",
      "Shape of inputs: torch.Size([1, 3, 4, 182, 182])\n",
      "Top prediction: beatboxing (0.38%)\n",
      "beatboxing 0.0037791572976857424\n"
     ]
    }
   ],
   "source": [
    "# RUNNING THE MODEL WITH OR WITHOUT WEBCAM\n",
    "# For webcam:\n",
    "process_videos(use_webcam=True)\n",
    "\n",
    "# For video file:\n",
    "# process_videos(video_path=\"C:/Users/nyok/Desktop/OpenCV/Videos/fixinghairlive.MOV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f497ce1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029a4d89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
