{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f897ec43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nyok\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\nyok\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\nyok\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS ALL PACKAGE\n",
    "\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import wget\n",
    "\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b72137e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\nyok/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\n"
     ]
    }
   ],
   "source": [
    "# DOWNLOAD THE MODEL\n",
    "\n",
    "# Device on which to run the model\n",
    "device = \"cpu\"\n",
    "\n",
    "# Pick a pretrained model and load the pretrained weights\n",
    "model_name = \"x3d_xs\"\n",
    "model_recognition = torch.hub.load(\"facebookresearch/pytorchvideo\", model=model_name, pretrained=True)\n",
    "\n",
    "# Set to eval mode and move to desired device\n",
    "model_recognition = model_recognition.eval()\n",
    "model_recognition = model_recognition.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7869bb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"sharpening knives\"', '\"eating ice cream\"', '\"cutting nails\"', '\"changing wheel\"', '\"bench pressing\"', 'deadlifting', '\"eating carrots\"', 'marching', '\"throwing discus\"', '\"playing flute\"'] (400,)\n"
     ]
    }
   ],
   "source": [
    "# DOWNLOAD THE KINETIC-400 LABEL\n",
    "\n",
    "# url = \"https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json\"\n",
    "# wget.download(url, 'kinetics_classnames.json')\n",
    "\n",
    "with open(\"kinetics_classnames.json\", \"r\") as f:\n",
    "    kinetics_classnames = json.load(f)\n",
    "    labels = [line.strip() for line in kinetics_classnames]\n",
    "\n",
    "# Create an id to label name mapping\n",
    "kinetics_id_to_classname = {}\n",
    "for k, v in kinetics_classnames.items():\n",
    "    kinetics_id_to_classname[v] = str(k).replace('\"', \"\")\n",
    "\n",
    "print(labels[0:10], np.shape(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "399e2a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORM PARAMETER\n",
    "mean = [0.45, 0.45, 0.45]\n",
    "std = [0.225, 0.225, 0.225]\n",
    "frames_per_second = 6\n",
    "\n",
    "# SPECIFIC PARAMETER FOR MODEL\n",
    "model_transform_params = {\n",
    "    \"x3d_xs\": {\"side_size\": 182, \"crop_size\": 182, \"num_frames\": 4, \"sampling_rate\": 12},\n",
    "    \"x3d_s\": {\"side_size\": 182, \"crop_size\": 182, \"num_frames\": 13, \"sampling_rate\": 6},\n",
    "    \"x3d_m\": {\"side_size\": 256, \"crop_size\": 256, \"num_frames\": 16, \"sampling_rate\": 5}\n",
    "}\n",
    "\n",
    "transform_params = model_transform_params[model_name]\n",
    "\n",
    "transform = ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=Compose(\n",
    "        [\n",
    "            UniformTemporalSubsample(transform_params[\"num_frames\"]),\n",
    "            Lambda(lambda x: x/255.0),\n",
    "            NormalizeVideo((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "            ShortSideScale(size=transform_params[\"side_size\"]),\n",
    "            CenterCropVideo(\n",
    "                crop_size=(transform_params[\"crop_size\"], transform_params[\"crop_size\"])\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# UniformTemporalSubsample : Reduces the number of frames to the required num_frames by uniformly sampling.\n",
    "# Lambda(lambda x: x/255.0): Normalizes pixel values to the range [0, 1].\n",
    "# NormalizeVideo           : Applies mean and standard deviation normalization using the provided values.\n",
    "# ShortSideScale           : Resizes the video frames so that the shorter side is of length side_size.\n",
    "# CenterCropVideo          : Crops the center crop_size√ócrop_size region from each frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87e22af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN PROCESSING FUNCTION\n",
    "\n",
    "def process_videosss(use_webcam=False, video_path=None):\n",
    "    # Initialize webcam or video file\n",
    "    if use_webcam:\n",
    "        capture = cv2.VideoCapture(0)\n",
    "    elif video_path:\n",
    "        capture = cv2.VideoCapture(video_path)\n",
    "    else:\n",
    "        print(\"Error: Provide video path or function for using webcam\")\n",
    "        return\n",
    "\n",
    "    if not capture.isOpened():\n",
    "        print(\"Error: Could not open video source.\")\n",
    "        return\n",
    "    \n",
    "    frame_buffer = []\n",
    "    \n",
    "    prev_time = 0\n",
    "    \n",
    "    frame_count = 0\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            ret, img = capture.read()\n",
    "            if not ret:\n",
    "                if not use_webcam:\n",
    "                    print(\"End of video file.\")\n",
    "                else:\n",
    "                    print(\"Error: Could not read frame from webcam.\")\n",
    "                break\n",
    "            \n",
    "            frame_count += 1\n",
    "            \n",
    "            # Skip frames based on the value of skip_frames\n",
    "            if frame_count % 2 != 0:\n",
    "                continue \n",
    "            \n",
    "            # Convert color format of openCV(BGR) to pytorch(RGB)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            current_time = time.time()\n",
    "            \n",
    "            # Add frame to buffer\n",
    "            frame_buffer.append(img)\n",
    "            if len(frame_buffer) < transform_params[\"num_frames\"]:\n",
    "                continue\n",
    "            \n",
    "            # Create a batch of repeated frames to match with required number for model\n",
    "            video_frames = frame_buffer[-transform_params[\"num_frames\"]:]\n",
    "            # Convert to PyTorch tensor and permuted the tensor into shape (C,H,T,W)\n",
    "            video_data = {\"video\": torch.tensor(np.array(video_frames)).permute(3, 0, 1, 2).float()}\n",
    "            \n",
    "            # Apply the transform to normalize the input for model\n",
    "            video_data = transform(video_data)\n",
    "            \n",
    "            # Move the inputs to the desired device\n",
    "            inputs = video_data[\"video\"].to(device)\n",
    "            \n",
    "            # Pass the input through the model\n",
    "            with torch.no_grad():\n",
    "                preds = model_recognition(inputs[None, ...])\n",
    "            \n",
    "            # Apply softmax to get class probabilities\n",
    "            post_act = torch.nn.Softmax(dim=1)\n",
    "            preds = post_act(preds)\n",
    "            pred_classes = preds.topk(k=5).indices[0]\n",
    "            \n",
    "            # Map the predicted classes to the label names\n",
    "            pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes]\n",
    "            \n",
    "            # Convert back color format from BGR to RGB\n",
    "            img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Calculate the FPS (frames per second)\n",
    "            fps = 1 / (current_time - prev_time)\n",
    "            prev_time = current_time\n",
    "\n",
    "            \n",
    "            # Display the prediction on the video frame\n",
    "            for i, class_name in enumerate(pred_class_names):\n",
    "                cv2.putText(img_bgr, f\"{class_name}\", (20, 40 + i * 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "                \n",
    "            cv2.putText(img_bgr, f\"FPS: {float(fps)}\", (20, 150), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 3)\n",
    "\n",
    "            # Show the video stream with predictions\n",
    "            cv2.imshow('X3D-S Action Recognition', img_bgr)\n",
    "                            \n",
    "            # Press 'Esc' to exit\n",
    "            if cv2.waitKey(30) & 0xFF == 27:\n",
    "                break\n",
    "\n",
    "    finally:\n",
    "        capture.release()\n",
    "        cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0721d1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_enhanced_roi(frame, roi):\n",
    "\n",
    "        # Draw corner brackets\n",
    "        cv2.line(frame, (roi[2] + 3, roi[0] + 3), (roi[2] + 3, roi[0] + 100), (0, 200, 0), 2)\n",
    "        cv2.line(frame, (roi[2] + 3, roi[0] + 3), (roi[2] + 100, roi[0] + 3), (0, 200, 0), 2)\n",
    "        cv2.line(frame, (roi[3] - 3, roi[1] - 3), (roi[3] - 3, roi[1] - 100), (0, 200, 0), 2)\n",
    "        cv2.line(frame, (roi[3] - 3, roi[1] - 3), (roi[3] - 100, roi[1] - 3), (0, 200, 0), 2)\n",
    "        cv2.line(frame, (roi[3] - 3, roi[0] + 3), (roi[3] - 3, roi[0] + 100), (0, 200, 0), 2)\n",
    "        cv2.line(frame, (roi[3] - 3, roi[0] + 3), (roi[3] - 100, roi[0] + 3), (0, 200, 0), 2)\n",
    "        cv2.line(frame, (roi[2] + 3, roi[1] - 3), (roi[2] + 3, roi[1] - 100), (0, 200, 0), 2)\n",
    "        cv2.line(frame, (roi[2] + 3, roi[1] - 3), (roi[2] + 100, roi[1] - 3), (0, 200, 0), 2)\n",
    "\n",
    "        # Write ROI label\n",
    "        FONT_STYLE = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        org = (roi[2] + 3, roi[1] - 3)\n",
    "        org2 = (roi[2] + 2, roi[1] - 2)\n",
    "        FONT_SIZE = 0.5\n",
    "        FONT_COLOR = (0, 200, 0)\n",
    "        FONT_COLOR2 = (0, 0, 0)\n",
    "        cv2.putText(frame, \"ROI\", org2, FONT_STYLE, FONT_SIZE, FONT_COLOR2, 2)\n",
    "        cv2.putText(frame, \"ROI\", org, FONT_STYLE, FONT_SIZE, FONT_COLOR, 2)\n",
    "\n",
    "        return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d2c5277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN PROCESSING FUNCTION\n",
    "def process_videos(use_webcam=False, video_path=None):\n",
    "    # Initialize webcam or video file\n",
    "    if use_webcam:\n",
    "        capture = cv2.VideoCapture(0)\n",
    "    elif video_path:\n",
    "        capture = cv2.VideoCapture(video_path)\n",
    "    else:\n",
    "        print(\"Error: Provide video path or function for using webcam\")\n",
    "        return\n",
    "    if not capture.isOpened():\n",
    "        print(\"Error: Could not open video source.\")\n",
    "        return\n",
    "    \n",
    "    frame_buffer = []\n",
    "    prev_time = 0\n",
    "    frame_count = 0\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            ret, img = capture.read()\n",
    "            if not ret:\n",
    "                if not use_webcam:\n",
    "                    print(\"End of video file.\")\n",
    "                else:\n",
    "                    print(\"Error: Could not read frame from webcam.\")\n",
    "                break\n",
    "            \n",
    "            frame_count += 1\n",
    "            \n",
    "            # Skip frames based on the value of skip_frames\n",
    "            if frame_count % 2 != 0:\n",
    "                continue \n",
    "            \n",
    "            # RESIZE THE IMAGE\n",
    "#             h, w, _ = img.shape\n",
    "#             scale = 2 / min(h, w)\n",
    "#             w_scaled, h_scaled = int(w * scale), int(h * scale)\n",
    "#             if w_scaled == w and h_scaled == h:\n",
    "#                 return img\n",
    "#             cv2.resize(img, (w_scaled, h_scaled))\n",
    "            \n",
    "            \n",
    "            # MAKE A CENTER CROP FOR ROI\n",
    "            def center_crop(img: np.ndarray) -> np.ndarray:\n",
    "                \"\"\"\n",
    "                Center crop squared the original frame to standardize the input image to the encoder model\n",
    "\n",
    "                :param frame: input frame\n",
    "                :returns: center-crop-squared frame\n",
    "                \"\"\"\n",
    "                img_h, img_w, _ = img.shape\n",
    "                min_dim = min(img_h, img_w)\n",
    "                start_x = int((img_w - min_dim) / 2.0)\n",
    "                start_y = int((img_h - min_dim) / 2.0)\n",
    "                roi = [start_y, (start_y + min_dim), start_x, (start_x + min_dim)]\n",
    "                return img[start_y : (start_y + min_dim), start_x : (start_x + min_dim), ...], roi\n",
    "\n",
    "            \n",
    "            # Convert color format of openCV(BGR) to pytorch(RGB)\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # MAKE THE ROI AS INPUT FOR PREDICTION\n",
    "            (img_crop, roi) = center_crop(img_rgb)\n",
    "            \n",
    "            current_time = time.time()\n",
    "            \n",
    "            # Add frame to buffer\n",
    "            frame_buffer.append(img_crop)\n",
    "            if len(frame_buffer) < transform_params[\"num_frames\"]:\n",
    "                continue\n",
    "            \n",
    "            # Create a batch of repeated frames to match with required number for model\n",
    "            video_frames = frame_buffer[-transform_params[\"num_frames\"]:]\n",
    "            # Convert to PyTorch tensor and permuted the tensor into shape (C,H,T,W)\n",
    "            video_data = {\"video\": torch.tensor(np.array(video_frames)).permute(3, 0, 1, 2).float()}\n",
    "            \n",
    "            # Apply the transform to normalize the input for model\n",
    "            video_data = transform(video_data)\n",
    "            \n",
    "            # Move the inputs to the desired device\n",
    "            inputs = video_data[\"video\"].to(device)\n",
    "            \n",
    "            # Pass the input through the model\n",
    "            with torch.no_grad():\n",
    "                preds = model(inputs[None, ...])\n",
    "            \n",
    "            # Apply softmax to get class probabilities\n",
    "            post_act = torch.nn.Softmax(dim=1)\n",
    "            preds = post_act(preds)\n",
    "            pred_classes = preds.topk(k=5).indices[0]\n",
    "            \n",
    "            # Map the predicted classes to the label names\n",
    "            pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes]\n",
    "            \n",
    "            # Calculate the FPS (frames per second)\n",
    "            fps = 1 / (current_time - prev_time)\n",
    "            prev_time = current_time\n",
    "            \n",
    "            # Display the prediction on the video frame\n",
    "            for i, class_name in enumerate(pred_class_names):\n",
    "                cv2.putText(img, f\"{class_name}\", (20, 40 + i * 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "                \n",
    "            cv2.putText(img, f\"FPS: {float(fps):.2f}\", (20, 150), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 3)\n",
    "            \n",
    "            # Draw enhanced ROI\n",
    "            img = draw_enhanced_roi(img, roi)\n",
    "            \n",
    "            # Show the video stream with predictions\n",
    "            cv2.imshow('X3D-S Action Recognition', img)\n",
    "                            \n",
    "            # Press 'Esc' to exit\n",
    "            if cv2.waitKey(30) & 0xFF == 27:\n",
    "                break\n",
    "    finally:\n",
    "        capture.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "# Usage examples:\n",
    "# For webcam:\n",
    "# process_video(use_webcam=True)\n",
    "\n",
    "# For video file:\n",
    "# process_video(video_path=\"path/to/your/video.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20a20e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d34cb035",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import torch\n",
    "from PIL import Image, ImageDraw\n",
    "from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import pil_to_tensor, to_pil_image\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "weights = FasterRCNN_MobileNet_V3_Large_320_FPN_Weights.DEFAULT\n",
    "model = fasterrcnn_mobilenet_v3_large_320_fpn(weights=weights, box_score_thresh=0.9)\n",
    "model.eval()\n",
    "\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# MAIN PROCESSING FUNCTION\n",
    "def process_vide(use_webcam=False, video_path=None):\n",
    "    # Initialize webcam or video file\n",
    "    if use_webcam:\n",
    "        capture = cv2.VideoCapture(0)\n",
    "    elif video_path:\n",
    "        capture = cv2.VideoCapture(video_path)\n",
    "    else:\n",
    "        print(\"Error: Provide video path or function for using webcam\")\n",
    "        return\n",
    "    if not capture.isOpened():\n",
    "        print(\"Error: Could not open video source.\")\n",
    "        return\n",
    "    \n",
    "    output_size = (224, 224)\n",
    "    frame_buffer = []\n",
    "    prev_time = 0\n",
    "    frame_count = 0\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            ret, img = capture.read()\n",
    "            if not ret:\n",
    "                print(\"Error: Could not read frame.\")\n",
    "                break\n",
    "            \n",
    "            frame_count += 1\n",
    "            \n",
    "            # Skip frames based on the value of skip_frames\n",
    "            if frame_count % 2 != 0:\n",
    "                continue \n",
    "            \n",
    "            resized_frame = cv2.resize(img, output_size)\n",
    "            # Convert OpenCV BGR frame to RGB\n",
    "            rgb_frame = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Convert numpy array to tensor, add batch dimension, and normalize\n",
    "            img_tensor = torch.from_numpy(rgb_frame).permute(2, 0, 1).float() / 255.0\n",
    "            img_tensor = img_tensor.unsqueeze(0)  # Add batch dimension\n",
    "            \n",
    "            print(f\"Shape of img_tensor: {img_tensor.shape}\")\n",
    "\n",
    "            # Apply inference preprocessing transforms\n",
    "            batch = [preprocess(img_tensor[0])]\n",
    "            \n",
    "            print(f\"Shape of batch[0]: {batch[0].shape}\")\n",
    "\n",
    "            # Step 5: Perform object detection\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    prediction = model(batch)\n",
    "                \n",
    "                print(f\"Number of predictions: {len(prediction)}\")\n",
    "                prediction = prediction[0]  # Get the first (and only) prediction\n",
    "                \n",
    "                print(f\"Keys in prediction: {prediction.keys()}\")\n",
    "                print(f\"Shape of prediction['boxes']: {prediction['boxes'].shape}\")\n",
    "                \n",
    "                # Extract labels and draw bounding boxes on the frame\n",
    "                boxes = prediction[\"boxes\"]\n",
    "                labels = prediction[\"labels\"]\n",
    "                scores = prediction[\"scores\"]\n",
    "\n",
    "                # Filter detections to keep only persons (class 1 in COCO dataset)\n",
    "                person_indices = labels == 1\n",
    "                person_boxes = boxes[person_indices]\n",
    "                person_scores = scores[person_indices]\n",
    "\n",
    "                print(f\"Number of detected persons: {len(person_boxes)}\")\n",
    "\n",
    "                # Draw bounding boxes on the original frame\n",
    "                result_frame = resized_frame.copy()\n",
    "                for box, score in zip(person_boxes, person_scores):\n",
    "                    if score > 0.7:  # Adjust confidence threshold as needed\n",
    "                        box = box.int().tolist()\n",
    "                        cv2.rectangle(result_frame, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
    "                        cv2.putText(result_frame, f\"Person {score:.2f}\", (box[0], box[1] - 10),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "                print(f\"Shape of result_frame: {result_frame.shape}\")\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                print(f\"RuntimeError occurred: {e}\")\n",
    "                print(f\"Error occurred at frame {frame_count}\")\n",
    "                break\n",
    "\n",
    "            \n",
    "            # Add frame to buffer\n",
    "            frame_buffer.append(cv2.cvtColor(result_frame, cv2.COLOR_BGR2RGB))\n",
    "            if len(frame_buffer) >= transform_params[\"num_frames\"]:\n",
    "                try:\n",
    "                    # Create a batch of repeated frames to match with required number for model\n",
    "                    video_frames = frame_buffer[-transform_params[\"num_frames\"]:]\n",
    "                    # Convert to PyTorch tensor and permute the tensor into shape (C,T,H,W)\n",
    "                    video_data = {\"video\": torch.tensor(np.array(video_frames)).permute(3, 1, 2, 0).float() / 255.0}\n",
    "\n",
    "                    print(f\"Shape of video_data['video']: {video_data['video'].shape}\")\n",
    "\n",
    "                    # Apply the transform to normalize the input for model\n",
    "                    video_data = transform(video_data)\n",
    "\n",
    "                    # Move the inputs to the desired device\n",
    "                    inputs = video_data[\"video\"].to(device)\n",
    "\n",
    "                    print(f\"Shape of inputs: {inputs.shape}\")\n",
    "\n",
    "                    # Pass the input through the model\n",
    "                    with torch.no_grad():\n",
    "                        preds = model(inputs.unsqueeze(0))  # Add batch dimension\n",
    "\n",
    "                    # Apply softmax to get class probabilities\n",
    "                    post_act = torch.nn.Softmax(dim=1)\n",
    "                    preds = post_act(preds)\n",
    "                    pred_classes = preds.topk(k=5).indices[0]\n",
    "\n",
    "                    # Map the predicted classes to the label names\n",
    "                    pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes]\n",
    "\n",
    "                    # Display the prediction on the video frame\n",
    "                    for i, class_name in enumerate(pred_class_names):\n",
    "                        cv2.putText(result_frame, f\"{class_name}\", (20, 40 + i * 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "                except RuntimeError as e:\n",
    "                    print(f\"RuntimeError in action recognition: {e}\")\n",
    "                    print(f\"Error occurred at frame {frame_count}\")\n",
    "        \n",
    "            current_time = time.time()\n",
    "\n",
    "            # Calculate FPS (frames per second)\n",
    "            fps = 1 / (current_time - prev_time)\n",
    "            prev_time = current_time\n",
    "        \n",
    "            cv2.putText(img, f\"FPS: {float(fps):.2f}\", (20, 150), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 3)\n",
    "            \n",
    "            \n",
    "            # Show the video stream with predictions\n",
    "            cv2.imshow('X3D-S Action Recognition', img)\n",
    "                            \n",
    "            # Press 'Esc' to exit\n",
    "            if cv2.waitKey(30) & 0xFF == 27:\n",
    "                break\n",
    "    finally:\n",
    "        capture.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "# Usage examples:\n",
    "# For webcam:\n",
    "# process_video(use_webcam=True)\n",
    "\n",
    "# For video file:\n",
    "# process_video(video_path=\"path/to/your/video.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1db10630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import queue\n",
    "import threading\n",
    "from collections import deque\n",
    "\n",
    "class VideoStreamProcessor:\n",
    "    def __init__(self, use_webcam=False, video_path=None, queue_size=128):\n",
    "        self.use_webcam = use_webcam\n",
    "        self.video_path = video_path\n",
    "        self.capture = None\n",
    "        \n",
    "        # Queues for thread communication\n",
    "        self.frame_queue = queue.Queue(maxsize=queue_size)\n",
    "        self.result_queue = queue.Queue(maxsize=queue_size)\n",
    "        self.frame_buffer = deque(maxlen=transform_params[\"num_frames\"])\n",
    "        \n",
    "        # Threading control\n",
    "        self.stopped = False\n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "    def start(self):\n",
    "        # Initialize capture\n",
    "        self.capture = cv2.VideoCapture(0 if self.use_webcam else self.video_path)\n",
    "        if not self.capture.isOpened():\n",
    "            raise RuntimeError(\"Could not open video source\")\n",
    "            \n",
    "        # Start threads\n",
    "        self.capture_thread = threading.Thread(target=self._capture_frames)\n",
    "        self.process_thread = threading.Thread(target=self._process_frames)\n",
    "        self.display_thread = threading.Thread(target=self._display_frames)\n",
    "        \n",
    "        self.capture_thread.start()\n",
    "        self.process_thread.start()\n",
    "        self.display_thread.start()\n",
    "        return self\n",
    "    \n",
    "    def stop(self):\n",
    "        self.stopped = True\n",
    "        # Wait for threads to finish\n",
    "        if self.capture_thread.is_alive():\n",
    "            self.capture_thread.join()\n",
    "        if self.process_thread.is_alive():\n",
    "            self.process_thread.join()\n",
    "        if self.display_thread.is_alive():\n",
    "            self.display_thread.join()\n",
    "            \n",
    "        if self.capture is not None:\n",
    "            self.capture.release()\n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "    def _capture_frames(self):\n",
    "        frame_count = 0\n",
    "        while not self.stopped:\n",
    "            if self.frame_queue.full():\n",
    "                time.sleep(0.1)  # Prevent busy-waiting\n",
    "                continue\n",
    "                \n",
    "            ret, frame = self.capture.read()\n",
    "            if not ret:\n",
    "                if not self.use_webcam:\n",
    "                    print(\"End of video file.\")\n",
    "                else:\n",
    "                    print(\"Error: Could not read frame from webcam.\")\n",
    "                self.stopped = True\n",
    "                break\n",
    "            \n",
    "            frame_count += 1\n",
    "            if frame_count % 2 == 0:  # Skip every other frame\n",
    "                # Convert BGR to RGB\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                self.frame_queue.put((frame_rgb, frame_count, time.time()))\n",
    "    \n",
    "    def _process_frames(self):\n",
    "        while not self.stopped:\n",
    "            try:\n",
    "                frame_data = self.frame_queue.get(timeout=1.0)\n",
    "            except queue.Empty:\n",
    "                continue\n",
    "                \n",
    "            frame_rgb, frame_count, timestamp = frame_data\n",
    "            \n",
    "            # Add to frame buffer\n",
    "            self.frame_buffer.append(frame_rgb)\n",
    "            \n",
    "            if len(self.frame_buffer) >= transform_params[\"num_frames\"]:\n",
    "                # Process batch of frames\n",
    "                video_frames = list(self.frame_buffer)\n",
    "                video_tensor = torch.tensor(np.array(video_frames)).permute(3, 0, 1, 2).float()\n",
    "                \n",
    "                # Apply transforms\n",
    "                video_data = {\"video\": video_tensor}\n",
    "                video_data = transform(video_data)\n",
    "                \n",
    "                # Move to device and get predictions\n",
    "                inputs = video_data[\"video\"].to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    preds = model(inputs[None, ...])\n",
    "                \n",
    "                # Get top predictions\n",
    "                post_act = torch.nn.Softmax(dim=1)\n",
    "                preds = post_act(preds)\n",
    "                pred_classes = preds.topk(k=5).indices[0]\n",
    "                pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes]\n",
    "                \n",
    "                # Convert back to BGR for display\n",
    "                frame_bgr = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n",
    "                \n",
    "                # Add predictions to frame\n",
    "                for i, class_name in enumerate(pred_class_names):\n",
    "                    cv2.putText(frame_bgr, \n",
    "                              f\"{class_name}\", \n",
    "                              (20, 40 + i * 20), \n",
    "                              cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                              0.6, \n",
    "                              (0, 255, 0), \n",
    "                              2)\n",
    "                \n",
    "                self.result_queue.put((frame_bgr, timestamp))\n",
    "            \n",
    "            self.frame_queue.task_done()\n",
    "    \n",
    "    def _display_frames(self):\n",
    "        prev_time = time.time()\n",
    "        \n",
    "        while not self.stopped:\n",
    "            try:\n",
    "                frame_bgr, timestamp = self.result_queue.get(timeout=1.0)\n",
    "            except queue.Empty:\n",
    "                continue\n",
    "                \n",
    "            # Calculate and display FPS\n",
    "            current_time = time.time()\n",
    "            fps = 1 / (current_time - prev_time)\n",
    "            prev_time = current_time\n",
    "            \n",
    "            cv2.putText(frame_bgr, \n",
    "                       f\"FPS: {float(fps):.2f}\", \n",
    "                       (20, 150), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                       0.7, \n",
    "                       (0, 255, 0), \n",
    "                       3)\n",
    "            \n",
    "            cv2.imshow('X3D-S Action Recognition', frame_bgr)\n",
    "            \n",
    "            if cv2.waitKey(1) & 0xFF == 27:\n",
    "                self.stopped = True\n",
    "                break\n",
    "                \n",
    "            self.result_queue.task_done()\n",
    "\n",
    "def process_video(use_webcam=False, video_path=None):\n",
    "    try:\n",
    "        # Start video processing pipeline\n",
    "        processor = VideoStreamProcessor(use_webcam=use_webcam, \n",
    "                                      video_path=video_path)\n",
    "        processor.start()\n",
    "        \n",
    "        # Wait for Esc key\n",
    "        while not processor.stopped:\n",
    "            time.sleep(0.1)\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted by user\")\n",
    "    finally:\n",
    "        processor.stop()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    process_video(use_webcam=True)  # For webcam\n",
    "    # process_video(video_path=\"path/to/video.mp4\")  # For video file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2efbcf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUNNING THE MODEL WITH OR WITHOUT WEBCAME\n",
    "# For webcam:\n",
    "# process_videosss(use_webcam=True)\n",
    "\n",
    "# For video file:\n",
    "process_videosss(video_path=\"C:/Users/nyok/Desktop/OpenCV/Videos/diving.MP4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba43250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aca48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMPAN\n",
    "\n",
    "# MAIN PROCESSING FUNCTION\n",
    "def process_video(use_webcam=False, video_path=None):\n",
    "    # Initialize webcam or video file\n",
    "    if use_webcam:\n",
    "        capture = cv2.VideoCapture(0)\n",
    "    elif video_path:\n",
    "        capture = cv2.VideoCapture(video_path)\n",
    "    else:\n",
    "        print(\"Error: Provide video path or function for using webcam\")\n",
    "        return\n",
    "    if not capture.isOpened():\n",
    "        print(\"Error: Could not open video source.\")\n",
    "        return\n",
    "    \n",
    "    input_size = (480, 480)\n",
    "    frame_buffer = []\n",
    "    frame_count = 0\n",
    "    prev_time = 0\n",
    "    start_time = time.time()   \n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            ret, img = capture.read()\n",
    "            if not ret:\n",
    "                print(\"Error: Could not read frame.\")\n",
    "                break\n",
    "            \n",
    "            frame_count += 1\n",
    "            \n",
    "            # Skip frames based on the value of skip_frames\n",
    "#             if frame_count % 2 != 0:\n",
    "#                 continue \n",
    "            \n",
    "            # Resize image based on input size\n",
    "            resized_frame = cv2.resize(img, input_size)\n",
    "            \n",
    "            # Call detection function \n",
    "            result_frame = detect(resized_frame)\n",
    "            \n",
    "            # Add frame to buffer\n",
    "            frame_buffer.append(cv2.cvtColor(result_frame, cv2.COLOR_BGR2RGB))\n",
    "            if len(frame_buffer) >= transform_params[\"num_frames\"]:\n",
    "                try:\n",
    "                    # Create a batch of repeated frames to match with required number for model\n",
    "                    video_frames = frame_buffer[-transform_params[\"num_frames\"]:]\n",
    "                    \n",
    "                    # Convert to PyTorch tensor and permute the tensor into shape (C,T,H,W)\n",
    "                    video_data = {\"video\": torch.tensor(np.array(video_frames)).permute(3, 0, 1, 2).float()}\n",
    "\n",
    "                    print(f\"Shape of video_data['video']: {video_data['video'].shape}\")\n",
    "\n",
    "                    # Apply the transform to normalize the input for model\n",
    "                    video_data = transform(video_data)\n",
    "                    \n",
    "                    predict_class = recognize(video_data)\n",
    "\n",
    "                    # Display the prediction on the video frame\n",
    "                    for i, class_name in enumerate(predict_class):\n",
    "                        cv2.putText(result_frame, f\"{class_name}\", (20, 40 + i * 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 153, 0), 2)\n",
    "                        \n",
    "                except RuntimeError as e:\n",
    "                    print(f\"RuntimeError in action recognition: {e}\")\n",
    "                    print(f\"Error occurred at frame {frame_count}\")\n",
    "        \n",
    "        \n",
    "            current_time = time.time()\n",
    "            total_time = current_time - start_time  # Total elapsed time since the start\n",
    "            \n",
    "            # Calculate FPS (instantaneous and average)\n",
    "            fps = 1 / (current_time - prev_time) if (current_time - prev_time) > 0 else 0\n",
    "            average_fps = frame_count / total_time if total_time > 0 else 0\n",
    "            prev_time = current_time\n",
    "    \n",
    "        \n",
    "            cv2.putText(result_frame, f\"FPS: {float(fps):.2f}\", (20, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "            cv2.putText(result_frame, f\"Average FPS: {float(average_fps):.2f}\", (20, 140), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "            \n",
    "            # Show the video stream with predictions\n",
    "            cv2.imshow('X3D-S Action Recognition', result_frame)\n",
    "                            \n",
    "            # Press 'Esc' to exit\n",
    "            if cv2.waitKey(30) & 0xFF == 27:\n",
    "                break\n",
    "    finally:\n",
    "        capture.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "def recognize(video_data):\n",
    "    # Move the inputs to the desired device\n",
    "    inputs = video_data[\"video\"].to(device)\n",
    "\n",
    "    print(f\"Shape of inputs: {inputs.shape}\")\n",
    "\n",
    "    # Pass the input through the model\n",
    "    with torch.no_grad():\n",
    "        preds = model_recognition(inputs[None, ...])\n",
    "\n",
    "    # Apply softmax to get class probabilities\n",
    "    post_act = torch.nn.Softmax(dim=1)\n",
    "    preds = post_act(preds)\n",
    "    pred_classes = preds.topk(k=3).indices[0]\n",
    "\n",
    "    # Map the predicted classes to the label names\n",
    "    pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes]\n",
    "    return pred_class_names\n",
    "\n",
    "\n",
    "\n",
    "# DETECTION FUNCTION\n",
    "def detect(resized_frame):\n",
    "    # Convert OpenCV BGR frame to PIL Image\n",
    "    pil_img = Image.fromarray(cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Convert PIL Image to tensor and add batch dimension\n",
    "    img_tensor = pil_to_tensor(pil_img).unsqueeze(0)\n",
    "\n",
    "    # Apply inference preprocessing transforms\n",
    "    batch = [preprocess(img_tensor[0])]\n",
    "\n",
    "    # Step 5: Perform object detection\n",
    "    with torch.no_grad():\n",
    "        prediction = model_detection(batch)[0]\n",
    "\n",
    "    # Draw bounding boxes on the frame\n",
    "    boxes = prediction[\"boxes\"]\n",
    "    box = draw_bounding_boxes(img_tensor[0], boxes=boxes, colors=\"red\", width=2)\n",
    "\n",
    "    # Convert tensor back to OpenCV format for display\n",
    "    result_frame = cv2.cvtColor(np.array(to_pil_image(box.detach())), cv2.COLOR_RGB2BGR)\n",
    "    return result_frame"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
