{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19a58103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nyok\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\nyok\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\nyok\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# IMPORT ALL PACKAGE\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import time\n",
    "import wget\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "from ipywidgets import Image\n",
    "\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47894432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\nyok/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\n"
     ]
    }
   ],
   "source": [
    "# DOWNLOAD MODEL\n",
    "\n",
    "# Device on which to run the model\n",
    "# Set to cuda to load on GPU\n",
    "device = \"cpu\"\n",
    "\n",
    "# Pick a pretrained model and load the pretrained weights\n",
    "model_name = \"slow_r50\"\n",
    "model = torch.hub.load(\"facebookresearch/pytorchvideo\", model=model_name, pretrained=True)\n",
    "\n",
    "# Set to eval mode and move to desired device\n",
    "model = model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5917d403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"sharpening knives\"', '\"eating ice cream\"', '\"cutting nails\"', '\"changing wheel\"', '\"bench pressing\"', 'deadlifting', '\"eating carrots\"', 'marching', '\"throwing discus\"', '\"playing flute\"'] (400,)\n"
     ]
    }
   ],
   "source": [
    "# DOWNLOAD KINETIC-400 LABEL\n",
    "# url = \"https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json\"\n",
    "# wget.download(url, 'kinetics_classnames.json')\n",
    "\n",
    "with open(\"kinetics_classnames.json\", \"r\") as f:\n",
    "    kinetics_classnames = json.load(f)\n",
    "    labels = [line.strip() for line in kinetics_classnames]\n",
    "\n",
    "# Create an id to label name mapping\n",
    "kinetics_id_to_classname = {}\n",
    "for k, v in kinetics_classnames.items():\n",
    "    kinetics_id_to_classname[v] = str(k).replace('\"', \"\")\n",
    "\n",
    "print(labels[0:10], np.shape(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcd28088",
   "metadata": {},
   "outputs": [],
   "source": [
    "side_size = 256\n",
    "mean = [0.45, 0.45, 0.45]\n",
    "std = [0.225, 0.225, 0.225]\n",
    "crop_size = 256\n",
    "num_frames = 8\n",
    "sampling_rate = 8\n",
    "frames_per_second = 16\n",
    "\n",
    "# Note that this transform is specific to the slow_R50 model.\n",
    "transform =  ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=Compose(\n",
    "        [\n",
    "            UniformTemporalSubsample(num_frames),\n",
    "            Lambda(lambda x: x/255.0),\n",
    "            NormalizeVideo(mean, std),\n",
    "            ShortSideScale(\n",
    "                size=side_size\n",
    "            ),\n",
    "            CenterCropVideo(crop_size=(crop_size, crop_size))\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# The duration of the input clip is also specific to the model.\n",
    "clip_duration = (num_frames * sampling_rate)/frames_per_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c016dece",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'current_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m     frames\u001b[38;5;241m.\u001b[39mappend(img)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Calculate the FPS (frames per second)\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m fps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (current_time \u001b[38;5;241m-\u001b[39m prev_time)\n\u001b[0;32m     22\u001b[0m prev_time \u001b[38;5;241m=\u001b[39m current_time\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(frames) \u001b[38;5;241m<\u001b[39m num_frames:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'current_time' is not defined"
     ]
    }
   ],
   "source": [
    "capture = cv2.VideoCapture(0)\n",
    "if not capture.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "prev_time = 0 \n",
    "    \n",
    "try:\n",
    "    while True:\n",
    "        frames = []\n",
    "        for _ in range(num_frames):\n",
    "            ret, img = capture.read()\n",
    "            if not ret:\n",
    "                print(\"Error: Could not read frame.\")\n",
    "                break\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.resize(img, (side_size, side_size))\n",
    "            frames.append(img)\n",
    "        \n",
    "        current_time = time.time()\n",
    "        # Calculate the FPS (frames per second)\n",
    "        fps = 1 / (current_time - prev_time)\n",
    "        prev_time = current_time\n",
    "\n",
    "        \n",
    "        if len(frames) < num_frames:\n",
    "            break\n",
    "        \n",
    "        # Create input tensor\n",
    "        video_tensor = torch.tensor(np.array(frames)).permute(3, 0, 1, 2).float()\n",
    "        video_data = {\"video\": video_tensor}\n",
    "        \n",
    "        # Apply the transform to normalize the input\n",
    "        video_data = transform(video_data)\n",
    "        \n",
    "        # Move the inputs to the desired device\n",
    "        inputs = video_data[\"video\"].to(device)\n",
    "        \n",
    "        # Pass the input through the model\n",
    "        with torch.no_grad():\n",
    "            preds = model(inputs[None, ...])\n",
    "        \n",
    "        \n",
    "        # Apply softmax to get class probabilities\n",
    "        post_act = torch.nn.Softmax(dim=1)\n",
    "        preds = post_act(preds)\n",
    "        pred_classes = preds.topk(k=5).indices[0]\n",
    "        \n",
    "        # Map the predicted classes to the label names\n",
    "        pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes]\n",
    "        \n",
    "        # Display the prediction on the video frame\n",
    "        img_bgr = cv2.cvtColor(frames[-1], cv2.COLOR_RGB2BGR)\n",
    "        for i, class_name in enumerate(pred_class_names):\n",
    "            cv2.putText(img_bgr, f\"{class_name}\", (20, 40 + i * 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "            \n",
    "        cv2.putText(img_bgr, f\"FPS: {int(fps)}\", (20, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 3)\n",
    "        \n",
    "        # Show the video stream with predictions\n",
    "        cv2.imshow('Slow Model Action Recognition', img_bgr)\n",
    "        \n",
    "        # Press 'Esc' to exit\n",
    "        if cv2.waitKey(30) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4804b155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
