{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ddca7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nyok\\anaconda3\\Lib\\site-packages\\torchvision\\utils.py:232: UserWarning: Argument 'font_size' will be ignored since 'font' is not set.\n",
      "  warnings.warn(\"Argument 'font_size' will be ignored since 'font' is not set.\")\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.io.image import decode_image\n",
    "from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image, pil_to_tensor\n",
    "\n",
    "# Step 1: Load the image using PIL\n",
    "img_path = \"C:/Users/nyok/Desktop/OpenCV/Photos/lady.jpg\"\n",
    "pil_img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "# Convert the PIL image to a tensor\n",
    "img_tensor = pil_to_tensor(pil_img).unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "# Step 2: Initialize model with the best available weights\n",
    "weights = FasterRCNN_MobileNet_V3_Large_320_FPN_Weights.DEFAULT\n",
    "model = fasterrcnn_mobilenet_v3_large_320_fpn(weights=weights, box_score_thresh=0.9)\n",
    "model.eval()\n",
    "\n",
    "# Step 3: Initialize the inference transforms\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Step 4: Apply inference preprocessing transforms\n",
    "batch = [preprocess(img_tensor[0])]\n",
    "\n",
    "# Step 5: Use the model and visualize the prediction\n",
    "with torch.no_grad():\n",
    "    prediction = model(batch)[0]\n",
    "\n",
    "labels = [weights.meta[\"categories\"][i] for i in prediction[\"labels\"]]\n",
    "box = draw_bounding_boxes(img_tensor[0], boxes=prediction[\"boxes\"],\n",
    "                          labels=labels,\n",
    "                          colors=\"red\", width=4, font_size=30)\n",
    "im = to_pil_image(box.detach())\n",
    "im.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af577446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import pil_to_tensor, to_pil_image\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "weights = FasterRCNN_MobileNet_V3_Large_320_FPN_Weights.DEFAULT\n",
    "model = fasterrcnn_mobilenet_v3_large_320_fpn(weights=weights, box_score_thresh=0.9)\n",
    "model.eval()\n",
    "\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Step 3: Open video file\n",
    "video_path = \"C:/Users/nyok/Desktop/OpenCV/Videos/archery.MP4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "output_size = (480, 480)\n",
    "\n",
    "# Check if video opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "# Step 4: Loop over video frames\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break  # Exit the loop when video ends\n",
    "    \n",
    "    resized_frame = cv2.resize(frame, output_size)\n",
    "    # Convert OpenCV BGR frame to PIL Image\n",
    "    pil_img = Image.fromarray(cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Convert PIL Image to tensor and add batch dimension\n",
    "    img_tensor = pil_to_tensor(pil_img).unsqueeze(0)\n",
    "\n",
    "    # Apply inference preprocessing transforms\n",
    "    batch = [preprocess(img_tensor[0])]\n",
    "\n",
    "    # Step 5: Perform object detection\n",
    "    with torch.no_grad():\n",
    "        prediction = model(batch)[0]\n",
    "\n",
    "    # Extract labels and draw bounding boxes on the frame\n",
    "    boxes = prediction[\"boxes\"]\n",
    "    box = draw_bounding_boxes(img_tensor[0], boxes=boxes, colors=\"red\", width=4, font_size=30)\n",
    "\n",
    "    # Convert tensor back to OpenCV format for display\n",
    "    result_frame = cv2.cvtColor(np.array(to_pil_image(box.detach())), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Step 6: Display the frame with bounding boxes\n",
    "    cv2.imshow('Object Detection', result_frame)\n",
    "\n",
    "    # Exit the video when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release video capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4639b2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webcam resolution: 640x480\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Step 1: Open webcam\n",
    "cap = cv2.VideoCapture(0)  # 0 is usually the default camera, change if necessary\n",
    "\n",
    "# Step 2: Check if the webcam is opened correctly\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not access the webcam.\")\n",
    "    exit()\n",
    "\n",
    "# Step 3: Get the default resolution of the webcam\n",
    "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "\n",
    "# Step 4: Print the resolution\n",
    "print(f\"Webcam resolution: {int(width)}x{int(height)}\")\n",
    "\n",
    "# Step 5: Release the webcam\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21750500",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 16.2 GiB for an array with shape (700, 2160, 3840, 3) and data type uint8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtransforms\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Step 1: Load the video\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m vid, _, _ \u001b[38;5;241m=\u001b[39m read_video(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/nyok/Desktop/OpenCV/Videos/fixinghair.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m vid \u001b[38;5;241m=\u001b[39m vid[:\u001b[38;5;241m32\u001b[39m]  \u001b[38;5;66;03m# Optionally shorten duration to the first 32 frames\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Check the shape of the video\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\io\\video.py:328\u001b[0m, in \u001b[0;36mread_video\u001b[1;34m(filename, start_pts, end_pts, pts_unit, output_format)\u001b[0m\n\u001b[0;32m    325\u001b[0m aframes_list \u001b[38;5;241m=\u001b[39m [frame\u001b[38;5;241m.\u001b[39mto_ndarray() \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m audio_frames]\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vframes_list:\n\u001b[1;32m--> 328\u001b[0m     vframes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(np\u001b[38;5;241m.\u001b[39mstack(vframes_list))\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     vframes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39muint8)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\core\\shape_base.py:471\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(arrays, axis, out, dtype, casting)\u001b[0m\n\u001b[0;32m    469\u001b[0m sl \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m),) \u001b[38;5;241m*\u001b[39m axis \u001b[38;5;241m+\u001b[39m (_nx\u001b[38;5;241m.\u001b[39mnewaxis,)\n\u001b[0;32m    470\u001b[0m expanded_arrays \u001b[38;5;241m=\u001b[39m [arr[sl] \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 471\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(expanded_arrays, axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout,\n\u001b[0;32m    472\u001b[0m                        dtype\u001b[38;5;241m=\u001b[39mdtype, casting\u001b[38;5;241m=\u001b[39mcasting)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 16.2 GiB for an array with shape (700, 2160, 3840, 3) and data type uint8"
     ]
    }
   ],
   "source": [
    "from torchvision.io.video import read_video\n",
    "from torchvision.models.video import mc3_18, MC3_18_Weights\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Step 1: Load the video\n",
    "vid, _, _ = read_video(\"C:/Users/nyok/Desktop/OpenCV/Videos/fixinghair.mp4\")\n",
    "vid = vid[:32]  # Optionally shorten duration to the first 32 frames\n",
    "\n",
    "# Check the shape of the video\n",
    "print(f\"Original shape of video: {vid.shape}\")  # [T, H, W, C]\n",
    "\n",
    "# Step 2: Initialize the model with the best available weights\n",
    "weights = MC3_18_Weights.DEFAULT\n",
    "model = mc3_18(weights=weights)\n",
    "model.eval()\n",
    "\n",
    "# Step 3: Initialize the inference transforms\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Step 4: Permute the video tensor to [T, C, H, W] from [T, H, W, C]\n",
    "vid = vid.permute(0, 3, 1, 2)  # Change to [T, C, H, W]\n",
    "print(f\"Permute shape of video: {vid.shape}\")\n",
    "\n",
    "# Step 5: Resize each frame to 224x224 (if necessary)\n",
    "resize_transform = transforms.Resize((224, 224))\n",
    "\n",
    "# Apply resize transform to each frame in the video\n",
    "vid_resized = torch.stack([resize_transform(frame) for frame in vid])\n",
    "\n",
    "# Step 6: Apply preprocessing transforms\n",
    "batch = preprocess(vid_resized).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Step 7: Use the model and print the predicted category\n",
    "with torch.no_grad():\n",
    "    prediction = model(batch).squeeze(0).softmax(0)\n",
    "    label = prediction.argmax().item()\n",
    "    score = prediction[label].item()\n",
    "    category_name = weights.meta[\"categories\"][label]\n",
    "    print(f\"{category_name}: {100 * score:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c5a1e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2f2b9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "Number of GPUs available: 0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA available:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of GPUs available:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count())\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent GPU:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mcurrent_device())\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU Name:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mget_device_name(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mcurrent_device()))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\cuda\\__init__.py:674\u001b[0m, in \u001b[0;36mcurrent_device\u001b[1;34m()\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcurrent_device\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m    673\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the index of a currently selected device.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     _lazy_init()\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_getDevice()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\cuda\\__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs available:\", torch.cuda.device_count())\n",
    "print(\"Current GPU:\", torch.cuda.current_device())\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303b12db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
